\documentclass[]{article}

\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\lstset{language=Python}

\title{Detecting Racism in Tweets : Progress Report}
\author{Fiete Botschen, Sean Noran}

\begin{document}

\maketitle

\onehalfspacing

\section{Goal Summary}
Our goal is to learn a model that can detect racism in text. In particular, we are training and evaluating our model on Tweets.

\section{Data Collection}

We are currently using a subset of a substantially large Twitter dataset. The data we used span over 20 days in January 2011 and 20 days in March 2015 and totals approximately 70 GB. This equates to nearly 30 million Tweets, including not only the Tweet itself but also various metadata pertaining to the Tweet, such as geolocation, Twitter account, number of times the Tweet was favorited, etc. The average Tweet contains about 30 characters and may contain links to images, news articles and other web content.

In addition to this available database of Tweets, we used a Twitter crawler to acquire a primarily racist dataset of Tweets by crawling a user that posts only racist content.

\section{Data Filtering}

The raw data is largely imbalanced; that is, racist Tweets occur infrequently. Thus generating an adequately large set of balanced data would be rather time-consuming. We therefore decide to filter the Tweets by words related to race. The list of words is not included in this report due to its explicit content.

For the data over January 2011, we filtered on explicit racial derogatory terms. In addition, we filtered the data on colors such as 'white' and 'black' and groups of people such as 'Mexicans' and 'Africans'. While many racist Tweets do include these terms, we found that most of the Tweets including these words were not considered racist, whereas the derogatory terms alone were better at isolating racism. This produced just over 70,000 Tweets.

For the data over March 2015, we filtered on only racial derogatory terms. This resulted in only around 10,000 Tweets to label.

\section{Data Labeling}

For the first dataset, we labeled 4000 Tweets manually ourselves and found 167 (4.175\%) of them to be racist.

For the second dataset, we labeled 4932 Tweets and found 426 (8.64\%) to be racist.

Because the labeling process was cumbersome for so few Tweets, we additionally decided to acquire data from a Twitter user who posts primarily racist content. We have over 6000 such Tweets, of which ... were labeled as racist.

\subsection{Subjective Labeling}

One of the major questions involved in the data collection process is: What is racism? 

The U.S. Commission on Civil Rights defines racism as "any action or attitude, conscious or unconscious, that subordinates an individual or group based on skin colour or race. It can be enacted individually or institutionally".

However, there is vast disagreement regarding how to define "race" and "subordinate". And indeed many alternative definitions of racism exist among various academic fields such as Sociology, Psychology, Political Science and Biology.

Because many people may disagree on which Tweets are racist, it would be better to define a confidence score as the percentage of people who said a given Tweet is racist. By this method, we would expect blatant racism to receive a confidence score close to 1, clearly non-racist Tweets to receive a score close to 0 and ambiguous Tweets to fall somewhere in between, close to 0.5.

However, it is also important to note that racism may be defined differently by different groups of people. In particular, an ethnic minority may find a certain Tweet to be racist, whereas a Caucasian may not see the racism in it.

Thus, if we are to collect data over a larger population of people, it would be important to account for many demographics. Additionally, we would want participants who themselves are not racist.

\section{Baseline}

For our baseline performance, text will be classified by instances from a pre-defined collection of words that are relevant to racism. That is, if a text contains any words that are relevant to racism, then it will be classified as racism. The obvious flaw in this is that racism is defined not by the individual words but by the meaning conveyed in the context surrounding those words.

\section{Preliminary Experiments}

Our first step is to get the data we are interested in. If we do not receive the data we requested, then we may have to label our own dataset manually. We may also create a small dataset regardless, while we wait for a response. We will then extract relevant features over text such as bag-of-words descriptors or bigram features, as described in previous work. Then we will train and evaluate a classifier using these features and see how it performs in comparison to our baseline approach. We expect to have these preliminary results in our progress report submission.

\end{document}